{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/ibrahim-syah/indonesian-distilbert-finetuned-squad/blob/main/notebook/Squad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"Copyright 2021 Rifky Bujana Bisri & Muhammad Fajrin Buyang Daffa\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\nCopyright 2023 Ibrahim Syah Qardhawi\n\nMade some changes to make it easier to look at the plot and change the pretrained model to Indonesian DistilBERT base.\n\nidk anything about licensing chatgpt told me to write it like this whatever","metadata":{"id":"uqMavf46syLy"}},{"cell_type":"code","source":"!pip install datasets evaluate\n!pip install transformers -U\n!pip install accelerate -U\n# To run the training on TPU, you will need to uncomment the following line:\n# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n!apt install git-lfs","metadata":{"id":"0Qd01fQGw_-n","outputId":"33ee511e-d0a3-4e73-b859-e654e8aa1ab2","execution":{"iopub.status.busy":"2023-10-02T06:57:46.626456Z","iopub.execute_input":"2023-10-02T06:57:46.626884Z","iopub.status.idle":"2023-10-02T06:58:34.842676Z","shell.execute_reply.started":"2023-10-02T06:57:46.626845Z","shell.execute_reply":"2023-10-02T06:58:34.841635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"KaEjYn7H0awt","outputId":"3c0d2b3a-bfd6-4f2e-d0d1-8fd644633123","execution":{"iopub.status.busy":"2023-10-02T06:58:34.844825Z","iopub.execute_input":"2023-10-02T06:58:34.845497Z","iopub.status.idle":"2023-10-02T06:58:34.849607Z","shell.execute_reply.started":"2023-10-02T06:58:34.845460Z","shell.execute_reply":"2023-10-02T06:58:34.848739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !git config --global user.email \"ibrahim.syah.q@gmail.com\"\n# !git config --global user.name \"ibrahim-syah\"","metadata":{"id":"mwwhzhbyxJnb","execution":{"iopub.status.busy":"2023-10-02T06:58:34.851368Z","iopub.execute_input":"2023-10-02T06:58:34.851723Z","iopub.status.idle":"2023-10-02T06:58:34.862849Z","shell.execute_reply.started":"2023-10-02T06:58:34.851694Z","shell.execute_reply":"2023-10-02T06:58:34.862097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\n\ntransformers.__version__","metadata":{"id":"YKXSWCQnxrzH","outputId":"6e8bf007-bae3-4424-bf42-ec116db85f3a","execution":{"iopub.status.busy":"2023-10-02T06:58:34.865567Z","iopub.execute_input":"2023-10-02T06:58:34.866088Z","iopub.status.idle":"2023-10-02T06:58:37.138444Z","shell.execute_reply.started":"2023-10-02T06:58:34.866053Z","shell.execute_reply":"2023-10-02T06:58:37.137549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"impossible_answer = True\nmodel_checkpoint = \"cahya/distilbert-base-indonesian\"\nbatch_size = 16","metadata":{"id":"IMzsbUQvymT2","execution":{"iopub.status.busy":"2023-10-02T06:58:37.139687Z","iopub.execute_input":"2023-10-02T06:58:37.140844Z","iopub.status.idle":"2023-10-02T06:58:37.145159Z","shell.execute_reply.started":"2023-10-02T06:58:37.140787Z","shell.execute_reply":"2023-10-02T06:58:37.144277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Runtime Settings","metadata":{"id":"nko8UMsbx01_"}},{"cell_type":"code","source":"# !nvidia-smi","metadata":{"id":"7wOu50vAx5eR","execution":{"iopub.status.busy":"2023-10-02T06:58:37.146734Z","iopub.execute_input":"2023-10-02T06:58:37.147542Z","iopub.status.idle":"2023-10-02T06:58:37.157532Z","shell.execute_reply.started":"2023-10-02T06:58:37.147511Z","shell.execute_reply":"2023-10-02T06:58:37.156785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{"id":"Np9VWTITxVT6"}},{"cell_type":"markdown","source":"## Load Dataset","metadata":{"id":"Uv4AeoK7xXCv"}},{"cell_type":"code","source":"import json\n\n#  # if using the qa-ind2 dataset in kaggle\ndev_filename = \"/kaggle/input/qa-ind2/dev-v2.0.json\"\ntrain_filename = \"/kaggle/input/qa-ind2/train-v2.0.json\"\n\n# # otherwise, download them yourself\n# !wget https://storage.depia.wiki/squad/tar/dev-v2.0.json\n# !wget https://storage.depia.wiki/squad/tar/train-v2.0.json\n# dev_filename = \"dev-v2.0.json\"\n# train_filename = \"train-v2.0.json\"\n\n# google colab default /content folder\n# dev_filename = \"/content/drive/MyDrive/dev-v2.0.json\"\n# train_filename = \"/content/drive/MyDrive/train-v2.0.json\"\n\nwith open(dev_filename) as f:\n    dev = json.load(f)\n\nwith open(train_filename) as f:\n    train = json.load(f)","metadata":{"id":"g5C-Y8pNvhY9","execution":{"iopub.status.busy":"2023-10-02T06:58:37.158997Z","iopub.execute_input":"2023-10-02T06:58:37.159315Z","iopub.status.idle":"2023-10-02T06:58:38.558985Z","shell.execute_reply.started":"2023-10-02T06:58:37.159287Z","shell.execute_reply":"2023-10-02T06:58:38.557970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ¤— does provide the [squad_v2](https://huggingface.co/datasets/squad_v2) dataset that has been formatted for training but that dataset was for the original SQuAD2.0 whereas the dataset we will use is the [Translated SQuAD2.0](https://github.com/Wikidepia/indonesian_datasets/tree/master/question-answering/squad).","metadata":{"id":"FHYJwdrXpdax"}},{"cell_type":"code","source":"print(\"There are %s data in this split\"%len(train[\"data\"])) # number of different data titles\n\ni = 3\ntitle = train[\"data\"][i][\"title\"]\nparagraphs = train[\"data\"][i][\"paragraphs\"]\nprint(\"The title for the data indexed at %s is %s\"%(i, title))\nprint(\"Said data title has %s paragraph(s)\"%(len(paragraphs)))\nprint(\"The first three paragraph is the following: \")\nparagraphs[:3]","metadata":{"id":"afcbhpOrpdax","outputId":"93b60658-0f96-4922-cd4d-fb050a7e9eb1","execution":{"iopub.status.busy":"2023-10-02T06:58:38.560475Z","iopub.execute_input":"2023-10-02T06:58:38.561210Z","iopub.status.idle":"2023-10-02T06:58:38.574448Z","shell.execute_reply.started":"2023-10-02T06:58:38.561176Z","shell.execute_reply":"2023-10-02T06:58:38.573504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's always a good idea to take a look at our data to get a feel of what they represent. If you compare this format with the finished one from ðŸ¤— squad_v2 you will see that ðŸ¤— squad_v2 pair together the question and answer along with the corresponding id, title, and context while removing everything else. Try looking at the dev split and see if you notice any difference ;).","metadata":{"id":"8trOB0FApday"}},{"cell_type":"markdown","source":"Since the this raw dataset has a different format than what was expected by our training model (we'll use ðŸ¤— AutoModelForQuestionAnswering)  we will need to format them so each pair of question and answer is an individual data.","metadata":{"id":"4-eFkk7jpday"}},{"cell_type":"code","source":"def format(content):\n    hf_data = []\n    for data in content[\"data\"]:\n        title = data[\"title\"]\n        for paragraph in data[\"paragraphs\"]:\n            context = paragraph[\"context\"]\n            for qa in paragraph[\"qas\"]:\n                fill = {\n                    \"id\":  qa[\"id\"],\n                    \"title\": title,\n                    \"context\": context,\n                    \"question\": qa[\"question\"],\n                    \"answers\": {\"answer_start\": [], \"text\": []}\n                }\n                if qa[\"is_impossible\"]:\n                    answers = qa[\"plausible_answers\"]\n                else:\n                    answers = qa[\"answers\"]\n                for answer in answers:\n                    fill[\"answers\"][\"answer_start\"].append(answer[\"answer_start\"])\n                    fill[\"answers\"][\"text\"].append(answer[\"text\"])\n\n                hf_data.append(fill)\n\n    return hf_data","metadata":{"id":"mhyjVMx_x9zB","execution":{"iopub.status.busy":"2023-10-02T06:58:38.576083Z","iopub.execute_input":"2023-10-02T06:58:38.576778Z","iopub.status.idle":"2023-10-02T06:58:38.584373Z","shell.execute_reply.started":"2023-10-02T06:58:38.576747Z","shell.execute_reply":"2023-10-02T06:58:38.583488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndev = format(dev)\ntrain = format(train)","metadata":{"id":"P_V4eweBEx-4","outputId":"44fb749c-301e-439b-b60e-d1843876043a","execution":{"iopub.status.busy":"2023-10-02T06:58:38.588332Z","iopub.execute_input":"2023-10-02T06:58:38.589105Z","iopub.status.idle":"2023-10-02T06:58:39.522890Z","shell.execute_reply.started":"2023-10-02T06:58:38.589075Z","shell.execute_reply":"2023-10-02T06:58:39.521929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # use small dataset for testing\n# train = train[:1000]\n# dev = dev[:100]\n# len(train), len(dev)","metadata":{"id":"t-O5pK3ypdaz","execution":{"iopub.status.busy":"2023-10-02T06:58:39.524458Z","iopub.execute_input":"2023-10-02T06:58:39.524791Z","iopub.status.idle":"2023-10-02T06:58:39.531542Z","shell.execute_reply.started":"2023-10-02T06:58:39.524759Z","shell.execute_reply":"2023-10-02T06:58:39.530484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import DatasetDict, Dataset\nimport pandas as pd\n\ndatasets = DatasetDict({\n    'train': Dataset.from_pandas(pd.DataFrame(train)),\n    'validation': Dataset.from_pandas(pd.DataFrame(dev))\n})\ndatasets","metadata":{"id":"y2iqSDm81486","outputId":"d39c4ded-9a03-4575-d09f-ae2b1c61f5f6","execution":{"iopub.status.busy":"2023-10-02T06:58:39.532956Z","iopub.execute_input":"2023-10-02T06:58:39.533371Z","iopub.status.idle":"2023-10-02T06:58:41.153544Z","shell.execute_reply.started":"2023-10-02T06:58:39.533339Z","shell.execute_reply":"2023-10-02T06:58:41.152610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datasets['train']['id'][0]","metadata":{"id":"e0sos8TW3SN5","outputId":"7f71f51f-4f5e-4ee7-e43e-15ef402a172e","execution":{"iopub.status.busy":"2023-10-02T06:58:41.155021Z","iopub.execute_input":"2023-10-02T06:58:41.155522Z","iopub.status.idle":"2023-10-02T06:58:41.260300Z","shell.execute_reply.started":"2023-10-02T06:58:41.155489Z","shell.execute_reply":"2023-10-02T06:58:41.259225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Analyzation","metadata":{"id":"mmYC-XHCFEhP"}},{"cell_type":"code","source":"import random\nimport numpy as np\n\n\ndef show_random_elements(dataset, num_examples=5):\n    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n    picks = []\n    for _ in range(num_examples):\n        pick = random.randint(0, len(dataset)-1)\n        while pick in picks:\n            pick = random.randint(0, len(dataset)-1)\n        picks.append(pick)\n\n    df = pd.DataFrame((np.asarray(dev)[picks]).tolist())\n\n    df['answer_start'] = [i['answer_start'] for i in df['answers']]\n    df['answer_text'] = [i['text'] for i in df['answers']]\n\n    del df['answers']\n\n    return df","metadata":{"id":"vPwJm5SuE3eZ","execution":{"iopub.status.busy":"2023-10-02T06:58:41.261939Z","iopub.execute_input":"2023-10-02T06:58:41.262309Z","iopub.status.idle":"2023-10-02T06:58:41.269248Z","shell.execute_reply.started":"2023-10-02T06:58:41.262276Z","shell.execute_reply":"2023-10-02T06:58:41.268408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are creating this dataframe to make them easier to work with in data analyzation. For training, we will still be using the original 'datasets' DatasetDict.","metadata":{"id":"Qq5YR3EHpda2"}},{"cell_type":"code","source":"from IPython.display import display, HTML\n\ndisplay(HTML(show_random_elements(dev).to_html()))","metadata":{"id":"Ok5GBgn-IHbB","outputId":"b8087236-89d6-4d24-f1d9-8aa1413b2577","execution":{"iopub.status.busy":"2023-10-02T06:58:41.270535Z","iopub.execute_input":"2023-10-02T06:58:41.271057Z","iopub.status.idle":"2023-10-02T06:58:41.298886Z","shell.execute_reply.started":"2023-10-02T06:58:41.271025Z","shell.execute_reply":"2023-10-02T06:58:41.297890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.DataFrame(train)\n\ntrain_df['answer_start'] = [i['answer_start'] for i in train_df['answers']]\ntrain_df['answer_text'] = [i['text'] for i in train_df['answers']]\n\ndel train_df['answers']","metadata":{"id":"CdauyQpkM5Zp","execution":{"iopub.status.busy":"2023-10-02T06:58:41.300397Z","iopub.execute_input":"2023-10-02T06:58:41.301061Z","iopub.status.idle":"2023-10-02T06:58:41.488208Z","shell.execute_reply.started":"2023-10-02T06:58:41.301017Z","shell.execute_reply":"2023-10-02T06:58:41.487265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dev_df = pd.DataFrame(dev)\n\ndev_df['answer_start'] = [i['answer_start'] for i in dev_df['answers']]\ndev_df['answer_text'] = [i['text'] for i in dev_df['answers']]\n\ndel dev_df['answers']","metadata":{"id":"ix5eCDZHNTFJ","execution":{"iopub.status.busy":"2023-10-02T06:58:41.489561Z","iopub.execute_input":"2023-10-02T06:58:41.489945Z","iopub.status.idle":"2023-10-02T06:58:41.512901Z","shell.execute_reply.started":"2023-10-02T06:58:41.489910Z","shell.execute_reply":"2023-10-02T06:58:41.511873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see how the length of each context, question, and answer for both of these splits.","metadata":{"id":"27ZQcdqbpda3"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Set up the figure and axes\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Define the datasets and their corresponding titles\nplot_datasets = [(train_df, \"Training Dataset\"), (dev_df, \"Validation Dataset\")]\n\n# Define the column names and corresponding titles\ncolumns = [('context', \"Context length histogram\"),\n           ('question', \"Question length histogram\"),\n           ('answer_text', \"Answer length histogram\")]\n\n# Loop through datasets and columns to plot histograms\nfor i, (dataset, dataset_title) in enumerate(plot_datasets):\n    for j, (column, title) in enumerate(columns):\n        ax = axes[i, j]\n        if column == \"answer_text\":\n            pd.DataFrame([len(n[0]) for n in dataset['answer_text']]).plot.hist(title=title, bins=20, ax=ax, grid=True)\n        else:\n            dataset[column].apply(len).plot.hist(title=title, bins=20, ax=ax, grid=True)\n        ax.set_xlabel(\"Length\")\n        ax.set_ylabel(\"Frequency\")\n        ax.grid(True)\n        if j == 0:\n            ax.set_ylabel(\"Frequency\")\n        if i == 1:\n            ax.set_xlabel(\"Length\")\n        if i == 0:\n            ax.set_title(title)\n#         if j == 0:\n#             ax.set_title(dataset_title)\n\n# Adjust the layout and show the plot\nplt.tight_layout()\nplt.show()","metadata":{"id":"sQcKzu98pda3","outputId":"c43c4060-3cca-4627-cd83-7184924674b6","execution":{"iopub.status.busy":"2023-10-02T06:58:41.514255Z","iopub.execute_input":"2023-10-02T06:58:41.515126Z","iopub.status.idle":"2023-10-02T06:58:43.323042Z","shell.execute_reply.started":"2023-10-02T06:58:41.515090Z","shell.execute_reply":"2023-10-02T06:58:43.322205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that the contexts, questions, and answers within the validation set (bottom) are shorter than their training counterpart, or maybe that's just because they lack more data.","metadata":{"id":"fOvLZof4pda3"}},{"cell_type":"markdown","source":"## Text Preprocessing","metadata":{"id":"uyaK2rFaQQly"}},{"cell_type":"markdown","source":"The ðŸ¤— [docs](https://huggingface.co/learn/nlp-course/chapter7/7?fw=pt#processing-the-training-data) has an excellent piece on this particular preprocessing part. In short, we will split long contexts into multiple features. Here is an example:","metadata":{"id":"TPjy4SvBpda4"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"id":"PoR5lsW_Pi0D","outputId":"70c49b23-461f-4701-874e-ceb1eb540b22","execution":{"iopub.status.busy":"2023-10-02T06:58:43.324040Z","iopub.execute_input":"2023-10-02T06:58:43.324387Z","iopub.status.idle":"2023-10-02T06:58:44.359544Z","shell.execute_reply.started":"2023-10-02T06:58:43.324354Z","shell.execute_reply":"2023-10-02T06:58:44.358654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nassert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)","metadata":{"id":"sgN_8VynQHE1","execution":{"iopub.status.busy":"2023-10-02T06:58:44.360961Z","iopub.execute_input":"2023-10-02T06:58:44.361519Z","iopub.status.idle":"2023-10-02T06:58:44.366619Z","shell.execute_reply.started":"2023-10-02T06:58:44.361487Z","shell.execute_reply":"2023-10-02T06:58:44.365720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = 384 # The maximum length of a feature (question and context)\ndoc_stride = 128 # The authorized overlap between two part of the context when splitting is needed.","metadata":{"id":"hkhTnYPlQPwm","execution":{"iopub.status.busy":"2023-10-02T06:58:44.368674Z","iopub.execute_input":"2023-10-02T06:58:44.369005Z","iopub.status.idle":"2023-10-02T06:58:44.380468Z","shell.execute_reply.started":"2023-10-02T06:58:44.368975Z","shell.execute_reply":"2023-10-02T06:58:44.379650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, example in enumerate(datasets[\"train\"]):\n    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n        break\nexample = datasets[\"train\"][i]","metadata":{"id":"SxhRS70MQXky","execution":{"iopub.status.busy":"2023-10-02T06:58:44.382368Z","iopub.execute_input":"2023-10-02T06:58:44.383745Z","iopub.status.idle":"2023-10-02T06:58:44.614836Z","shell.execute_reply.started":"2023-10-02T06:58:44.383677Z","shell.execute_reply":"2023-10-02T06:58:44.613891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])","metadata":{"id":"zFHTVsPaQ27c","outputId":"1a963aea-e051-46ba-a1c5-e58ecaa8cb19","execution":{"iopub.status.busy":"2023-10-02T06:58:44.615986Z","iopub.execute_input":"2023-10-02T06:58:44.616536Z","iopub.status.idle":"2023-10-02T06:58:44.625070Z","shell.execute_reply.started":"2023-10-02T06:58:44.616497Z","shell.execute_reply":"2023-10-02T06:58:44.624115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(tokenizer(example[\"question\"], example[\"context\"], max_length=max_length, truncation=\"only_second\")[\"input_ids\"])","metadata":{"id":"KB3jDIZ-Rm5n","outputId":"5c00c6df-3640-465a-a60a-71c17ac57b1d","execution":{"iopub.status.busy":"2023-10-02T06:58:44.626699Z","iopub.execute_input":"2023-10-02T06:58:44.627316Z","iopub.status.idle":"2023-10-02T06:58:44.637616Z","shell.execute_reply.started":"2023-10-02T06:58:44.627285Z","shell.execute_reply":"2023-10-02T06:58:44.636537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_example = tokenizer(\n    example[\"question\"],\n    example[\"context\"],\n    max_length=max_length,\n    truncation=\"only_second\",\n    return_overflowing_tokens=True,\n    stride=doc_stride\n)","metadata":{"id":"bdL3B0zhRt0G","execution":{"iopub.status.busy":"2023-10-02T06:58:44.638755Z","iopub.execute_input":"2023-10-02T06:58:44.639488Z","iopub.status.idle":"2023-10-02T06:58:44.649827Z","shell.execute_reply.started":"2023-10-02T06:58:44.639459Z","shell.execute_reply":"2023-10-02T06:58:44.648989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[len(x) for x in tokenized_example[\"input_ids\"]]","metadata":{"id":"anvg4tEHRwX2","outputId":"651961d9-b4dd-48ec-e64a-d20d06a8a6c3","execution":{"iopub.status.busy":"2023-10-02T06:58:44.651340Z","iopub.execute_input":"2023-10-02T06:58:44.651983Z","iopub.status.idle":"2023-10-02T06:58:44.661773Z","shell.execute_reply.started":"2023-10-02T06:58:44.651952Z","shell.execute_reply":"2023-10-02T06:58:44.660774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x in tokenized_example[\"input_ids\"][:]:\n    print(tokenizer.decode(x))","metadata":{"id":"L0H5_AJtRyNj","outputId":"8b43d048-bd8c-4c79-c890-92d7b8ddfa08","execution":{"iopub.status.busy":"2023-10-02T06:58:44.662844Z","iopub.execute_input":"2023-10-02T06:58:44.663888Z","iopub.status.idle":"2023-10-02T06:59:00.419084Z","shell.execute_reply.started":"2023-10-02T06:58:44.663856Z","shell.execute_reply":"2023-10-02T06:59:00.418056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compared to the original:","metadata":{"id":"skHLmuOApda8"}},{"cell_type":"code","source":"example","metadata":{"id":"H0Li-pj9pda8","outputId":"bf7249d0-d8de-466b-8a8f-53a2137c4ce4","execution":{"iopub.status.busy":"2023-10-02T06:59:00.423951Z","iopub.execute_input":"2023-10-02T06:59:00.425140Z","iopub.status.idle":"2023-10-02T06:59:00.430696Z","shell.execute_reply.started":"2023-10-02T06:59:00.425102Z","shell.execute_reply":"2023-10-02T06:59:00.429821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_example = tokenizer(\n    example[\"question\"],\n    example[\"context\"],\n    max_length=max_length,\n    truncation=\"only_second\",\n    return_overflowing_tokens=True,\n    return_offsets_mapping=True,\n    stride=doc_stride\n)\nprint(tokenized_example[\"offset_mapping\"][1][:100])","metadata":{"id":"kiGrnsRVR00s","outputId":"eadb43e5-8782-40dc-8e97-732b72bf54f2","execution":{"iopub.status.busy":"2023-10-02T06:59:00.432081Z","iopub.execute_input":"2023-10-02T06:59:00.432662Z","iopub.status.idle":"2023-10-02T06:59:00.446306Z","shell.execute_reply.started":"2023-10-02T06:59:00.432632Z","shell.execute_reply":"2023-10-02T06:59:00.445250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_token_id = tokenized_example[\"input_ids\"][0][1]\noffsets = tokenized_example[\"offset_mapping\"][0][1]\nprint(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])","metadata":{"id":"Ka7GEpZKR7KA","outputId":"0594ec7a-8fe1-4795-bb5e-7c3251aae415","execution":{"iopub.status.busy":"2023-10-02T06:59:00.447761Z","iopub.execute_input":"2023-10-02T06:59:00.448337Z","iopub.status.idle":"2023-10-02T06:59:00.456821Z","shell.execute_reply.started":"2023-10-02T06:59:00.448308Z","shell.execute_reply":"2023-10-02T06:59:00.455690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because of the feature split, we need to be careful when looking if the answer is within the context fully or not. We can use sequence_ids() to help us.","metadata":{"id":"sdV5-MUWpda-"}},{"cell_type":"code","source":"sequence_ids = tokenized_example.sequence_ids()\nprint(sequence_ids)","metadata":{"id":"OV3zkWKBSAvs","outputId":"1929a6e9-b18e-4f79-92cd-b4bb6b43372c","execution":{"iopub.status.busy":"2023-10-02T06:59:00.457902Z","iopub.execute_input":"2023-10-02T06:59:00.458511Z","iopub.status.idle":"2023-10-02T06:59:00.470295Z","shell.execute_reply.started":"2023-10-02T06:59:00.458480Z","shell.execute_reply":"2023-10-02T06:59:00.469162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see an example where we tokenize four examples with max_length of 100 and a stride of 50 so it will produce more features (more likely for a feature to not contain our answer).","metadata":{"id":"up1vAEt7pda_"}},{"cell_type":"code","source":"inputs = tokenizer(\n    datasets[\"train\"][2:6][\"question\"],\n    datasets[\"train\"][2:6][\"context\"],\n    max_length=100,\n    truncation=\"only_second\",\n    stride=50,\n    return_overflowing_tokens=True,\n    return_offsets_mapping=True,\n)\n\nprint(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\nprint(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")","metadata":{"id":"wzJ8b2kPpda_","outputId":"ce05bb6f-675d-4841-eb53-9863d3a35757","execution":{"iopub.status.busy":"2023-10-02T06:59:00.471665Z","iopub.execute_input":"2023-10-02T06:59:00.472418Z","iopub.status.idle":"2023-10-02T06:59:00.489792Z","shell.execute_reply.started":"2023-10-02T06:59:00.472387Z","shell.execute_reply":"2023-10-02T06:59:00.488752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answers = datasets[\"train\"][2:6][\"answers\"]\nstart_positions = []\nend_positions = []\n\nfor i, offset in enumerate(inputs[\"offset_mapping\"]):\n    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n    answer = answers[sample_idx]\n    start_char = answer[\"answer_start\"][0]\n    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n    sequence_ids = inputs.sequence_ids(i)\n\n    # Find the start and end of the context\n    idx = 0\n    while sequence_ids[idx] != 1:\n        idx += 1\n    context_start = idx\n    while sequence_ids[idx] == 1:\n        idx += 1\n    context_end = idx - 1\n\n    # If the answer is not fully inside the context, label is (0, 0)\n    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n        start_positions.append(0)\n        end_positions.append(0)\n    else:\n        # Otherwise it's the start and end token positions\n        idx = context_start\n        while idx <= context_end and offset[idx][0] <= start_char:\n            idx += 1\n        start_positions.append(idx - 1)\n\n        idx = context_end\n        while idx >= context_start and offset[idx][1] >= end_char:\n            idx -= 1\n        end_positions.append(idx + 1)\n\nstart_positions, end_positions","metadata":{"id":"0ks3O8Xvpda_","outputId":"af016a0d-2e39-40af-ae31-361ee7d7cded","execution":{"iopub.status.busy":"2023-10-02T06:59:00.491121Z","iopub.execute_input":"2023-10-02T06:59:00.491795Z","iopub.status.idle":"2023-10-02T06:59:00.503953Z","shell.execute_reply.started":"2023-10-02T06:59:00.491764Z","shell.execute_reply":"2023-10-02T06:59:00.503011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Which means we didn't find the answer in the 0th and the 5th feature so we just denote that with the (0, 0) tuple or [CLS] token.","metadata":{"id":"1x1Y10wmpdbA"}},{"cell_type":"code","source":"idx = 1\nsample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\nanswer = answers[sample_idx][\"text\"][0]\n\nstart = start_positions[idx]\nend = end_positions[idx]\nlabeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n\nprint(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")","metadata":{"id":"xeAgbsYhpdbA","outputId":"ad154582-a424-499b-f113-5672e251b36b","execution":{"iopub.status.busy":"2023-10-02T06:59:00.505227Z","iopub.execute_input":"2023-10-02T06:59:00.505720Z","iopub.status.idle":"2023-10-02T06:59:00.521680Z","shell.execute_reply.started":"2023-10-02T06:59:00.505692Z","shell.execute_reply":"2023-10-02T06:59:00.520710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = 5\nsample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\nanswer = answers[sample_idx][\"text\"][0]\n\nstart = start_positions[idx]\nend = end_positions[idx]\nlabeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\ndecoded_example = tokenizer.decode(inputs[\"input_ids\"][idx])\nprint(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")\nprint(\"decoded example: \",decoded_example)","metadata":{"id":"Su0HXg8MpdbA","outputId":"bb85a156-26ea-4e1c-9cbb-7ba79b07aa2e","execution":{"iopub.status.busy":"2023-10-02T06:59:00.523069Z","iopub.execute_input":"2023-10-02T06:59:00.523610Z","iopub.status.idle":"2023-10-02T06:59:00.535813Z","shell.execute_reply.started":"2023-10-02T06:59:00.523581Z","shell.execute_reply":"2023-10-02T06:59:00.534806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can start preprocessing the training set.","metadata":{"id":"AKjS6J62pdbB"}},{"cell_type":"code","source":"max_length = 384\nstride = 128\n\n\ndef preprocess_training_examples(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace with strip()\n    questions = [q.strip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = inputs.pop(\"offset_mapping\")\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    answers = examples[\"answers\"]\n    start_positions = []\n    end_positions = []\n\n    for i, offset in enumerate(offset_mapping):\n        sample_idx = sample_map[i]\n        answer = answers[sample_idx]\n        start_char = answer[\"answer_start\"][0]\n        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = inputs.sequence_ids(i)\n\n        # Find the start and end of the context\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n\n        # If the answer is not fully inside the context, label is (0, 0) or [CLS]\n        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            # Otherwise it's the start and end token positions\n            idx = context_start\n            while idx <= context_end and offset[idx][0] <= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n\n            idx = context_end\n            while idx >= context_start and offset[idx][1] >= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs","metadata":{"id":"uxX1TncLpdbB","execution":{"iopub.status.busy":"2023-10-02T06:59:00.537397Z","iopub.execute_input":"2023-10-02T06:59:00.537791Z","iopub.status.idle":"2023-10-02T06:59:00.551084Z","shell.execute_reply.started":"2023-10-02T06:59:00.537762Z","shell.execute_reply":"2023-10-02T06:59:00.550272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = datasets.map(\n    preprocess_training_examples,\n    batched=True,\n    remove_columns=datasets[\"train\"].column_names,\n)\ntokenized_datasets","metadata":{"id":"pD7LPs0SpdbB","outputId":"bce94812-952b-41e2-a64c-b4ee1d173079","execution":{"iopub.status.busy":"2023-10-02T06:59:00.552446Z","iopub.execute_input":"2023-10-02T06:59:00.553245Z","iopub.status.idle":"2023-10-02T07:00:33.969778Z","shell.execute_reply.started":"2023-10-02T06:59:00.553197Z","shell.execute_reply":"2023-10-02T07:00:33.968895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can get to fine-tuning","metadata":{"id":"PGMs-HL5pdbK"}},{"cell_type":"markdown","source":"## Fine Tuning Model","metadata":{"id":"TilHuKX_8ce3"}},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering, AutoModel, TrainingArguments, Trainer\n\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n\n# model_checkpoint = \"boimbukanbaim/distilbert-indonesian-squad\"\n# model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)","metadata":{"id":"xmh3nPLE8CmF","outputId":"767a7945-4b35-41f9-ccd4-51a28aba1d95","execution":{"iopub.status.busy":"2023-10-02T07:08:14.442194Z","iopub.execute_input":"2023-10-02T07:08:14.442667Z","iopub.status.idle":"2023-10-02T07:08:22.103808Z","shell.execute_reply.started":"2023-10-02T07:08:14.442628Z","shell.execute_reply":"2023-10-02T07:08:22.102874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from huggingface_hub import notebook_login\n\n# notebook_login()","metadata":{"id":"u8I6q8sdpdbL","outputId":"b4f6007b-23de-4e7a-a678-fe11e19071bc","execution":{"iopub.status.busy":"2023-10-02T07:08:32.256384Z","iopub.execute_input":"2023-10-02T07:08:32.256728Z","iopub.status.idle":"2023-10-02T07:08:32.260785Z","shell.execute_reply.started":"2023-10-02T07:08:32.256701Z","shell.execute_reply":"2023-10-02T07:08:32.259882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\nargs = TrainingArguments(\n    \"distilbert-indonesian-squad\",\n    evaluation_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    report_to=\"none\",\n#     push_to_hub=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:08:43.125686Z","iopub.execute_input":"2023-10-02T07:08:43.126017Z","iopub.status.idle":"2023-10-02T07:08:43.213247Z","shell.execute_reply.started":"2023-10-02T07:08:43.125990Z","shell.execute_reply":"2023-10-02T07:08:43.212353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import default_data_collator\n\ndata_collator = default_data_collator","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:08:49.053743Z","iopub.execute_input":"2023-10-02T07:08:49.054092Z","iopub.status.idle":"2023-10-02T07:08:49.059070Z","shell.execute_reply.started":"2023-10-02T07:08:49.054064Z","shell.execute_reply":"2023-10-02T07:08:49.058010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:08:50.380446Z","iopub.execute_input":"2023-10-02T07:08:50.380793Z","iopub.status.idle":"2023-10-02T07:08:58.787148Z","shell.execute_reply.started":"2023-10-02T07:08:50.380747Z","shell.execute_reply":"2023-10-02T07:08:58.786181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T04:05:38.459825Z","iopub.execute_input":"2023-10-02T04:05:38.460181Z","iopub.status.idle":"2023-10-02T04:05:46.928068Z","shell.execute_reply.started":"2023-10-02T04:05:38.460153Z","shell.execute_reply":"2023-10-02T04:05:46.926490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainer.save_model(\"distilbert-indonesian-squad\")\n# trainer.push_to_hub(commit_message=\"Training complete\")","metadata":{"id":"qqXwa_Tb4NRb","outputId":"b5569333-646a-4243-8833-f4cab2e989e2","execution":{"iopub.status.busy":"2023-10-02T04:04:26.235777Z","iopub.status.idle":"2023-10-02T04:04:26.236807Z","shell.execute_reply.started":"2023-10-02T04:04:26.236517Z","shell.execute_reply":"2023-10-02T04:04:26.236544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Simple Evaluation (EM & F1)\n\nBefore we get to the evaluation, we need to consider the post-processing. The model will output logits for the start and end positions of the answer in the input IDs. A usual ðŸ¤— pipeline will compute the score by softmaxing the logits and taking the product to each start-end pair and then pick the best valid answer.\n\nWe don't need to compute actual scores to make training faster. So we don't need to softmax and taking the product of the start-end pair. We also won't take all the scores but only 20 best scores. \n\nThe way we take the score from the pair is just by taking the sum. We can do this because we skipped the softmax so our output is still logits and\n$\\log{(ab)} = \\log{(a)} + \\log{(b)}$","metadata":{}},{"cell_type":"markdown","source":"Let's demonstrate this by making a prediction from a subset of our validation dataset with a trained model. The model that we will use is a finetuned distilbert from ðŸ¤—. We can preprocess said dataset with the tokenizer provided by the trained model.","metadata":{}},{"cell_type":"markdown","source":"However, before that we need to tweak the preprocessing for this validation dataset to add an `example_id` feature so that we can map the newly created features to the original context.\n\nIn contrast to the ```preprocess_training_examples```, We can use the validation set to interpret the model's prediction into spans of the original contex.","metadata":{}},{"cell_type":"code","source":"def preprocess_validation_examples(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    example_ids = []\n\n    for i in range(len(inputs[\"input_ids\"])):\n        # collect feature(s) belonging to the same id\n        sample_idx = sample_map[i]\n        example_ids.append(examples[\"id\"][sample_idx])\n\n        # turn all non-context token to None\n        sequence_ids = inputs.sequence_ids(i)\n        offset = inputs[\"offset_mapping\"][i]\n        inputs[\"offset_mapping\"][i] = [\n            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n        ]\n\n    inputs[\"example_id\"] = example_ids\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:09:09.462982Z","iopub.execute_input":"2023-10-02T07:09:09.463366Z","iopub.status.idle":"2023-10-02T07:09:09.471896Z","shell.execute_reply.started":"2023-10-02T07:09:09.463337Z","shell.execute_reply":"2023-10-02T07:09:09.470942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_dataset = datasets[\"validation\"].map(\n    preprocess_validation_examples,\n    batched=True,\n    remove_columns=datasets[\"validation\"].column_names,\n)\nvalidation_dataset","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:09:12.115492Z","iopub.execute_input":"2023-10-02T07:09:12.115854Z","iopub.status.idle":"2023-10-02T07:10:10.381516Z","shell.execute_reply.started":"2023-10-02T07:09:12.115826Z","shell.execute_reply":"2023-10-02T07:10:10.380285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Slightly different from the dataset that we use in training but this one can be used for our simple evaluation. Now we can get to the evaluation. First we will see how well the original distilbert model by ðŸ¤— performs on this validation dataset.","metadata":{}},{"cell_type":"code","source":"small_eval_set = datasets[\"validation\"].select(range(100)) # only 100 because I keep getting out of memory error\ntrained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n\n# since preprocess_validation_examples uses tokenizer namespace\n# we will change the tokenizer temporarily\ntokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\neval_set = small_eval_set.map(\n    preprocess_validation_examples,\n    batched=True,\n    remove_columns=datasets[\"validation\"].column_names,\n)\neval_set","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:10:37.942140Z","iopub.execute_input":"2023-10-02T07:10:37.943143Z","iopub.status.idle":"2023-10-02T07:10:39.723694Z","shell.execute_reply.started":"2023-10-02T07:10:37.943100Z","shell.execute_reply":"2023-10-02T07:10:39.722726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's immediately change the tokenizer variable back to the one we're using before we forgot.","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:10:41.920386Z","iopub.execute_input":"2023-10-02T07:10:41.920749Z","iopub.status.idle":"2023-10-02T07:10:42.455587Z","shell.execute_reply.started":"2023-10-02T07:10:41.920721Z","shell.execute_reply":"2023-10-02T07:10:42.454670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can then remove some unnecessary columns (for feeding them into the model) and just feed our data in batches if using GPU.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForQuestionAnswering\n\neval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\neval_set_for_model.set_format(\"torch\")\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nbatch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\ntrained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(\n    device\n)\n\nbatch","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:10:45.332986Z","iopub.execute_input":"2023-10-02T07:10:45.333355Z","iopub.status.idle":"2023-10-02T07:10:48.232999Z","shell.execute_reply.started":"2023-10-02T07:10:45.333326Z","shell.execute_reply":"2023-10-02T07:10:48.231898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    outputs = trained_model(**batch)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:10:51.437560Z","iopub.execute_input":"2023-10-02T07:10:51.437906Z","iopub.status.idle":"2023-10-02T07:10:55.335496Z","shell.execute_reply.started":"2023-10-02T07:10:51.437878Z","shell.execute_reply":"2023-10-02T07:10:55.334560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll change the output into numpy arrays.","metadata":{}},{"cell_type":"code","source":"start_logits = outputs.start_logits.cpu().numpy()\nend_logits = outputs.end_logits.cpu().numpy()\n\nstart_logits.shape, end_logits.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:10:58.433827Z","iopub.execute_input":"2023-10-02T07:10:58.434166Z","iopub.status.idle":"2023-10-02T07:10:58.442104Z","shell.execute_reply.started":"2023-10-02T07:10:58.434136Z","shell.execute_reply":"2023-10-02T07:10:58.441065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems like from the original 100 subset from the validation set, it has turned into 124 features each with 384 output logits of start-end pairs. Next up is mapping the predicted answer for each of these feature to their corresponding example.","metadata":{}},{"cell_type":"code","source":"import collections\n\nexample_to_features = collections.defaultdict(list)\nfor idx, feature in enumerate(eval_set):\n    example_to_features[feature[\"example_id\"]].append(idx)\n    \nexample_to_features","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:11:00.651846Z","iopub.execute_input":"2023-10-02T07:11:00.652175Z","iopub.status.idle":"2023-10-02T07:11:00.800152Z","shell.execute_reply.started":"2023-10-02T07:11:00.652149Z","shell.execute_reply":"2023-10-02T07:11:00.799091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll use a simple loop to go through each example with their associated features. We'll then look at the 20 best logit scores that are valid. An invalid prediction would be ones that aren't inside their context, an answer with negative length, or an answer that's too long.\n\nAfter which we can pick the one with the best score.","metadata":{}},{"cell_type":"code","source":"n_best = 20\nmax_answer_length = 30\npredicted_answers = []\n\nfor example in small_eval_set:\n    example_id = example[\"id\"]\n    context = example[\"context\"]\n    answers = []\n\n    for feature_index in example_to_features[example_id]:\n        start_logit = start_logits[feature_index]\n        end_logit = end_logits[feature_index]\n        offsets = eval_set[\"offset_mapping\"][feature_index]\n\n        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n        for start_index in start_indexes:\n            for end_index in end_indexes:\n                # Skip answers that are not fully in the context\n                if offsets[start_index] is None or offsets[end_index] is None:\n                    continue\n                # Skip answers with a length that is either < 0 or > max_answer_length.\n                if (\n                    end_index < start_index\n                    or end_index - start_index + 1 > max_answer_length\n                ):\n                    continue\n\n                answers.append(\n                    {\n                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n                    }\n                )\n\n    best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n    predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:11:04.343990Z","iopub.execute_input":"2023-10-02T07:11:04.345059Z","iopub.status.idle":"2023-10-02T07:11:19.689560Z","shell.execute_reply.started":"2023-10-02T07:11:04.345019Z","shell.execute_reply":"2023-10-02T07:11:19.688460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We formatted ``predicted_answer`` with the format that is expected for the metric provided by ðŸ¤— Evaluate library. Let's see if it performs well with the original squad metric.","metadata":{}},{"cell_type":"code","source":"import evaluate\n\nmetric = evaluate.load(\"squad\")","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:11:19.691476Z","iopub.execute_input":"2023-10-02T07:11:19.692499Z","iopub.status.idle":"2023-10-02T07:11:22.473361Z","shell.execute_reply.started":"2023-10-02T07:11:19.692462Z","shell.execute_reply":"2023-10-02T07:11:22.472379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This particular `squad` metric expects the predicted answer in the format of list of dict with one key for example ID and one key for the predicted text while the format of the theoretical answer in the format below:","metadata":{}},{"cell_type":"code","source":"theoretical_answers = [\n    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n]","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:11:22.474712Z","iopub.execute_input":"2023-10-02T07:11:22.475616Z","iopub.status.idle":"2023-10-02T07:11:22.494153Z","shell.execute_reply.started":"2023-10-02T07:11:22.475584Z","shell.execute_reply":"2023-10-02T07:11:22.493225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(predicted_answers[7])\nprint(theoretical_answers[7])","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:11:22.496173Z","iopub.execute_input":"2023-10-02T07:11:22.496815Z","iopub.status.idle":"2023-10-02T07:11:22.502849Z","shell.execute_reply.started":"2023-10-02T07:11:22.496785Z","shell.execute_reply":"2023-10-02T07:11:22.501858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And here's the metric of all the small subset of validated set:","metadata":{}},{"cell_type":"code","source":"metric.compute(predictions=predicted_answers, references=theoretical_answers)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:11:22.504264Z","iopub.execute_input":"2023-10-02T07:11:22.504788Z","iopub.status.idle":"2023-10-02T07:11:22.536813Z","shell.execute_reply.started":"2023-10-02T07:11:22.504758Z","shell.execute_reply":"2023-10-02T07:11:22.535933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This model (distilbert-base-cased-distilled-squad by ðŸ¤—) was not trained on Indonesian dataset, hence why our scores are poor. Let's put everything in compute_metrics().\n\nNormally, this function will be included in the ``Trainer`` object and will be used during the training loop. But here we'll just make a simple function that we can use at the end of training to see how well our model performs.","metadata":{}},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\n\ndef compute_metrics(start_logits, end_logits, features, examples):\n    example_to_features = collections.defaultdict(list)\n    for idx, feature in enumerate(features):\n        example_to_features[feature[\"example_id\"]].append(idx)\n\n    predicted_answers = []\n    for example in tqdm(examples):\n        example_id = example[\"id\"]\n        context = example[\"context\"]\n        answers = []\n\n        # Loop through all features associated with that example\n        for feature_index in example_to_features[example_id]:\n            start_logit = start_logits[feature_index]\n            end_logit = end_logits[feature_index]\n            offsets = features[feature_index][\"offset_mapping\"]\n\n            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Skip answers that are not fully in the context\n                    if offsets[start_index] is None or offsets[end_index] is None:\n                        continue\n                    # Skip answers with a length that is either < 0 or > max_answer_length\n                    if (\n                        end_index < start_index\n                        or end_index - start_index + 1 > max_answer_length\n                    ):\n                        continue\n\n                    answer = {\n                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n                    }\n                    answers.append(answer)\n\n        # Select the answer with the best score\n        if len(answers) > 0:\n            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n            predicted_answers.append(\n                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n            )\n        else:\n            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n\n    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n    return metric.compute(predictions=predicted_answers, references=theoretical_answers)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:11:22.538065Z","iopub.execute_input":"2023-10-02T07:11:22.538575Z","iopub.status.idle":"2023-10-02T07:11:22.548168Z","shell.execute_reply.started":"2023-10-02T07:11:22.538540Z","shell.execute_reply":"2023-10-02T07:11:22.547392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see if our model [distilbert-indonesian-squad](https://huggingface.co/boimbukanbaim/distilbert-indonesian-squad) would do the trick:","metadata":{}},{"cell_type":"code","source":"small_eval_set = datasets[\"validation\"].select(range(100))\n# small_eval_set = datasets[\"validation\"]\ntrained_checkpoint = \"boimbukanbaim/distilbert-indonesian-squad\" # our trained model\n\n# since preprocess_validation_examples uses tokenizer namespace\n# we will change the tokenizer temporarily\ntokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\neval_set = small_eval_set.map(\n    preprocess_validation_examples,\n    batched=True,\n    remove_columns=datasets[\"validation\"].column_names,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\neval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\neval_set_for_model.set_format(\"torch\")\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nbatch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\ntrained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(\n    device\n)\n\nwith torch.no_grad():\n    outputs = trained_model(**batch)\n    \nstart_logits = outputs.start_logits.cpu().numpy()\nend_logits = outputs.end_logits.cpu().numpy()\n\ncompute_metrics(start_logits, end_logits, eval_set, small_eval_set)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:11:29.236947Z","iopub.execute_input":"2023-10-02T07:11:29.237308Z","iopub.status.idle":"2023-10-02T07:11:32.629943Z","shell.execute_reply.started":"2023-10-02T07:11:29.237281Z","shell.execute_reply":"2023-10-02T07:11:32.628992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Much better! what if we use the metric for SQuAD2.0? We will have to change the formatting to this:\n\nPredictions : List of triple for question-answers to score with the following key-value pairs:\n\n* 'id': the question-answer identification field of the question and answer pair\n* 'prediction_text' : the text of the answer\n* 'no_answer_probability' : the probability that the question has no answer\n\nReferences: List of question-answers dictionaries with the following key-value pairs:\n\n* 'id': id of the question-answer pair (see above),\n* 'answers': a list of Dict {'text': text of the answer as a string}\n* 'no_answer_threshold': the probability threshold to decide that a questionÂ hasÂ noÂ answer.","metadata":{}},{"cell_type":"markdown","source":"## SQuADv2.0 Evaluation (WIP)","metadata":{"id":"iVqsnGkuw_Tl"}},{"cell_type":"code","source":"# import torch\n\n# for batch in trainer.get_eval_dataloader():\n#     break\n# batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n# with torch.no_grad():\n#     output = trainer.model(**batch)\n# output.keys()","metadata":{"id":"7dDrMAwR3Dsc","execution":{"iopub.status.busy":"2023-10-02T07:11:43.771332Z","iopub.execute_input":"2023-10-02T07:11:43.771687Z","iopub.status.idle":"2023-10-02T07:11:43.844142Z","shell.execute_reply.started":"2023-10-02T07:11:43.771657Z","shell.execute_reply":"2023-10-02T07:11:43.843061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output.start_logits.shape, output.end_logits.shape","metadata":{"id":"MNwUC0k33El4","execution":{"iopub.status.busy":"2023-10-02T07:11:50.412356Z","iopub.execute_input":"2023-10-02T07:11:50.413117Z","iopub.status.idle":"2023-10-02T07:11:50.420076Z","shell.execute_reply.started":"2023-10-02T07:11:50.413084Z","shell.execute_reply":"2023-10-02T07:11:50.418124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)","metadata":{"id":"5st0SF2F3J3h","execution":{"iopub.status.busy":"2023-10-02T07:11:54.311572Z","iopub.execute_input":"2023-10-02T07:11:54.312662Z","iopub.status.idle":"2023-10-02T07:11:54.324997Z","shell.execute_reply.started":"2023-10-02T07:11:54.312629Z","shell.execute_reply":"2023-10-02T07:11:54.323865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# n_best_size = 20","metadata":{"id":"CD2eKWtE3MOR","execution":{"iopub.status.busy":"2023-10-02T07:12:06.300925Z","iopub.execute_input":"2023-10-02T07:12:06.301542Z","iopub.status.idle":"2023-10-02T07:12:06.306249Z","shell.execute_reply.started":"2023-10-02T07:12:06.301496Z","shell.execute_reply":"2023-10-02T07:12:06.305295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n\n# start_logits = output.start_logits[0].cpu().numpy()\n# end_logits = output.end_logits[0].cpu().numpy()\n# # Gather the indices the best start/end logits:\n# start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n# end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n# valid_answers = []\n# for start_index in start_indexes:\n#     for end_index in end_indexes:\n#         if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n#             valid_answers.append(\n#                 {\n#                     \"score\": start_logits[start_index] + end_logits[end_index],\n#                     \"text\": \"\" # We need to find a way to get back the original substring corresponding to the answer in the context\n#                 }\n#             )","metadata":{"id":"nXqwgcXD3OV_","execution":{"iopub.status.busy":"2023-10-02T07:12:10.327691Z","iopub.execute_input":"2023-10-02T07:12:10.328052Z","iopub.status.idle":"2023-10-02T07:12:10.335774Z","shell.execute_reply.started":"2023-10-02T07:12:10.328024Z","shell.execute_reply":"2023-10-02T07:12:10.334712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pad_on_right = True","metadata":{"id":"Z7yPG2jh3jRA","execution":{"iopub.status.busy":"2023-10-02T07:12:11.434501Z","iopub.execute_input":"2023-10-02T07:12:11.434848Z","iopub.status.idle":"2023-10-02T07:12:11.439646Z","shell.execute_reply.started":"2023-10-02T07:12:11.434810Z","shell.execute_reply":"2023-10-02T07:12:11.438718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def prepare_validation_features(examples):\n#     # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n#     # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n#     # left whitespace\n#     examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n#     # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n#     # in one example possible giving several features when a context is long, each of those features having a\n#     # context that overlaps a bit the context of the previous feature.\n#     tokenized_examples = tokenizer(\n#         examples[\"question\" if pad_on_right else \"context\"],\n#         examples[\"context\" if pad_on_right else \"question\"],\n#         truncation=\"only_second\" if pad_on_right else \"only_first\",\n#         max_length=max_length,\n#         stride=doc_stride,\n#         return_overflowing_tokens=True,\n#         return_offsets_mapping=True,\n#         padding=\"max_length\",\n#     )\n\n#     # Since one example might give us several features if it has a long context, we need a map from a feature to\n#     # its corresponding example. This key gives us just that.\n#     sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n#     # We keep the example_id that gave us this feature and we will store the offset mappings.\n#     tokenized_examples[\"example_id\"] = []\n\n#     for i in range(len(tokenized_examples[\"input_ids\"])):\n#         # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n#         sequence_ids = tokenized_examples.sequence_ids(i)\n#         context_index = 1 if pad_on_right else 0\n\n#         # One example can give several spans, this is the index of the example containing this span of text.\n#         sample_index = sample_mapping[i]\n#         tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n\n#         # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n#         # position is part of the context or not.\n#         tokenized_examples[\"offset_mapping\"][i] = [\n#             (o if sequence_ids[k] == context_index else None)\n#             for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n#         ]\n\n#     return tokenized_examples\n","metadata":{"id":"zUrVtsz13UHu","execution":{"iopub.status.busy":"2023-10-02T07:12:12.977568Z","iopub.execute_input":"2023-10-02T07:12:12.977922Z","iopub.status.idle":"2023-10-02T07:12:12.985650Z","shell.execute_reply.started":"2023-10-02T07:12:12.977895Z","shell.execute_reply":"2023-10-02T07:12:12.984700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# small_eval_set = datasets[\"validation\"].select(range(100))\n\n# validation_features = small_eval_set.map(\n#     prepare_validation_features,\n#     batched=True,\n#     remove_columns=datasets[\"validation\"].column_names\n# )\n\n# validation_features","metadata":{"id":"hkedUgKV3qq_","execution":{"iopub.status.busy":"2023-10-02T07:12:20.716606Z","iopub.execute_input":"2023-10-02T07:12:20.716958Z","iopub.status.idle":"2023-10-02T07:12:21.202963Z","shell.execute_reply.started":"2023-10-02T07:12:20.716928Z","shell.execute_reply":"2023-10-02T07:12:21.202038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# torch.cuda.empty_cache() ","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:12:25.561484Z","iopub.execute_input":"2023-10-02T07:12:25.562179Z","iopub.status.idle":"2023-10-02T07:12:25.575936Z","shell.execute_reply.started":"2023-10-02T07:12:25.562148Z","shell.execute_reply":"2023-10-02T07:12:25.574756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import AutoModelForQuestionAnswering, AutoModel, TrainingArguments, Trainer\n\n# trained_checkpoint = \"boimbukanbaim/distilbert-indonesian-squad\"\n# model = AutoModel.from_pretrained(trained_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:12:34.486330Z","iopub.execute_input":"2023-10-02T07:12:34.486857Z","iopub.status.idle":"2023-10-02T07:12:35.920301Z","shell.execute_reply.started":"2023-10-02T07:12:34.486802Z","shell.execute_reply":"2023-10-02T07:12:35.919369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import TrainingArguments, Trainer\n\n# batch_size = 1\n\n# args = TrainingArguments(\n#     \"distilbert-indonesian-squad\",\n#     evaluation_strategy = \"epoch\",\n#     save_strategy = \"epoch\",\n#     learning_rate=2e-5,\n#     per_device_train_batch_size=batch_size,\n#     per_device_eval_batch_size=batch_size,\n#     num_train_epochs=3,\n#     weight_decay=0.01,\n#     report_to=\"none\",\n# #     push_to_hub=True,\n# )\n\n# trainer = Trainer(\n#     model,\n#     args,\n#     train_dataset=tokenized_datasets[\"train\"],\n#     eval_dataset=tokenized_datasets[\"validation\"],\n#     data_collator=data_collator,\n#     tokenizer=tokenizer,\n# )","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:12:42.586384Z","iopub.execute_input":"2023-10-02T07:12:42.586752Z","iopub.status.idle":"2023-10-02T07:12:42.697807Z","shell.execute_reply.started":"2023-10-02T07:12:42.586724Z","shell.execute_reply":"2023-10-02T07:12:42.696675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # fix the out of memory error\n# raw_predictions = trainer.predict(validation_features)","metadata":{"id":"GD3Qiuui3tFd","execution":{"iopub.status.busy":"2023-10-02T07:12:46.448671Z","iopub.execute_input":"2023-10-02T07:12:46.449005Z","iopub.status.idle":"2023-10-02T07:12:52.754533Z","shell.execute_reply.started":"2023-10-02T07:12:46.448979Z","shell.execute_reply":"2023-10-02T07:12:52.753479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))","metadata":{"id":"pmrlMHmX3tzw","execution":{"iopub.status.busy":"2023-10-02T07:12:55.944911Z","iopub.execute_input":"2023-10-02T07:12:55.945283Z","iopub.status.idle":"2023-10-02T07:12:55.951686Z","shell.execute_reply.started":"2023-10-02T07:12:55.945253Z","shell.execute_reply":"2023-10-02T07:12:55.950627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_answer_length = 30","metadata":{"id":"NW9wj6Xh3wKu","execution":{"iopub.status.busy":"2023-10-02T07:12:57.583484Z","iopub.execute_input":"2023-10-02T07:12:57.583827Z","iopub.status.idle":"2023-10-02T07:12:57.588107Z","shell.execute_reply.started":"2023-10-02T07:12:57.583790Z","shell.execute_reply":"2023-10-02T07:12:57.587167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start_logits = output.start_logits[0].cpu().numpy()\n# end_logits = output.end_logits[0].cpu().numpy()\n# offset_mapping = validation_features[0][\"offset_mapping\"]\n# # The first feature comes from the first example. For the more general case, we will need to be match the example_id to\n# # an example index\n# context = datasets[\"validation\"][0][\"context\"]\n\n# # Gather the indices the best start/end logits:\n# start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n# end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n# valid_answers = []\n# for start_index in start_indexes:\n#     for end_index in end_indexes:\n#         # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n#         # to part of the input_ids that are not in the context.\n#         if (\n#             start_index >= len(offset_mapping)\n#             or end_index >= len(offset_mapping)\n#             or offset_mapping[start_index] is None\n#             or offset_mapping[end_index] is None\n#         ):\n#             continue\n#         # Don't consider answers with a length that is either < 0 or > max_answer_length.\n#         if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n#             continue\n#         if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n#             start_char = offset_mapping[start_index][0]\n#             end_char = offset_mapping[end_index][1]\n#             valid_answers.append(\n#                 {\n#                     \"score\": start_logits[start_index] + end_logits[end_index],\n#                     \"text\": context[start_char: end_char]\n#                 }\n#             )\n\n# valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n# valid_answers","metadata":{"id":"CRwcOiND30Z5","execution":{"iopub.status.busy":"2023-10-02T07:13:02.602307Z","iopub.execute_input":"2023-10-02T07:13:02.602642Z","iopub.status.idle":"2023-10-02T07:13:02.618603Z","shell.execute_reply.started":"2023-10-02T07:13:02.602616Z","shell.execute_reply":"2023-10-02T07:13:02.617588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# valid_answers[5]","metadata":{"id":"IWFAp4Rd31Lx","execution":{"iopub.status.busy":"2023-10-02T07:13:05.340817Z","iopub.execute_input":"2023-10-02T07:13:05.341198Z","iopub.status.idle":"2023-10-02T07:13:05.347963Z","shell.execute_reply.started":"2023-10-02T07:13:05.341168Z","shell.execute_reply":"2023-10-02T07:13:05.347002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# datasets[\"validation\"][1231]","metadata":{"id":"Z9UXCeqH33gZ","execution":{"iopub.status.busy":"2023-10-02T07:13:10.633749Z","iopub.execute_input":"2023-10-02T07:13:10.634105Z","iopub.status.idle":"2023-10-02T07:13:10.642003Z","shell.execute_reply.started":"2023-10-02T07:13:10.634079Z","shell.execute_reply":"2023-10-02T07:13:10.641051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import collections\n\n# examples = datasets[\"validation\"]\n# features = validation_features\n\n# example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n# features_per_example = collections.defaultdict(list)\n# for i, feature in enumerate(features):\n#     features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)","metadata":{"id":"AJbD3kFm35Rb","execution":{"iopub.status.busy":"2023-10-02T07:13:29.489888Z","iopub.execute_input":"2023-10-02T07:13:29.490252Z","iopub.status.idle":"2023-10-02T07:13:29.604554Z","shell.execute_reply.started":"2023-10-02T07:13:29.490201Z","shell.execute_reply":"2023-10-02T07:13:29.603672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tqdm.auto import tqdm\n\n# def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n#     all_start_logits, all_end_logits = raw_predictions\n#     # Build a map example to its corresponding features.\n#     example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n#     features_per_example = collections.defaultdict(list)\n#     for i, feature in enumerate(features):\n#         features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n#     # The dictionaries we have to fill.\n#     predictions = collections.OrderedDict()\n\n#     # Logging.\n#     print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n#     # Let's loop over all the examples!\n#     for example_index, example in enumerate(tqdm(examples)):\n#         # Those are the indices of the features associated to the current example.\n#         feature_indices = features_per_example[example_index]\n\n#         min_null_score = None # Only used if squad_v2 is True.\n#         valid_answers = []\n\n#         context = example[\"context\"]\n#         # Looping through all the features associated to the current example.\n#         for feature_index in feature_indices:\n#             # We grab the predictions of the model for this feature.\n#             start_logits = all_start_logits[feature_index]\n#             end_logits = all_end_logits[feature_index]\n#             # This is what will allow us to map some the positions in our logits to span of texts in the original\n#             # context.\n#             offset_mapping = features[feature_index][\"offset_mapping\"]\n\n#             # Update minimum null prediction.\n#             cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n#             feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n#             if min_null_score is None or min_null_score < feature_null_score:\n#                 min_null_score = feature_null_score\n\n#             # Go through all possibilities for the `n_best_size` greater start and end logits.\n#             start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n#             end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n#             for start_index in start_indexes:\n#                 for end_index in end_indexes:\n#                     # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n#                     # to part of the input_ids that are not in the context.\n#                     if (\n#                         start_index >= len(offset_mapping)\n#                         or end_index >= len(offset_mapping)\n#                         or offset_mapping[start_index] is None\n#                         or offset_mapping[end_index] is None\n#                     ):\n#                         continue\n#                     # Don't consider answers with a length that is either < 0 or > max_answer_length.\n#                     if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n#                         continue\n\n#                     start_char = offset_mapping[start_index][0]\n#                     end_char = offset_mapping[end_index][1]\n#                     valid_answers.append(\n#                         {\n#                             \"score\": start_logits[start_index] + end_logits[end_index],\n#                             \"text\": context[start_char: end_char]\n#                         }\n#                     )\n\n#         if len(valid_answers) > 0:\n#             best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n#         else:\n#             # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n#             # failure.\n#             best_answer = {\"text\": \"\", \"score\": 0.0}\n\n#         # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n#         if not impossible_answer:\n#             predictions[example[\"id\"]] = best_answer[\"text\"]\n#         else:\n#             answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n#             predictions[example[\"id\"]] = answer\n\n#     return predictions","metadata":{"id":"NPum2ZXm380G","execution":{"iopub.status.busy":"2023-10-02T07:17:47.571521Z","iopub.execute_input":"2023-10-02T07:17:47.572084Z","iopub.status.idle":"2023-10-02T07:17:47.592508Z","shell.execute_reply.started":"2023-10-02T07:17:47.572043Z","shell.execute_reply":"2023-10-02T07:17:47.591300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final_predictions = postprocess_qa_predictions(small_eval_set, validation_features, raw_predictions.predictions)","metadata":{"id":"NiOExw5t4BEQ","execution":{"iopub.status.busy":"2023-10-02T07:17:48.430469Z","iopub.execute_input":"2023-10-02T07:17:48.430838Z","iopub.status.idle":"2023-10-02T07:17:50.705448Z","shell.execute_reply.started":"2023-10-02T07:17:48.430812Z","shell.execute_reply":"2023-10-02T07:17:50.704422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def print_answer(id):\n#     text = [i for i in datasets['validation'] if i['id'] == id][0]\n#     print(f\"Text: {text['context']}\")\n#     print(f\"Question: {text['question']}\")\n#     print(f\"Answer: {final_predictions[id]}\")","metadata":{"id":"kxHLw_9w4HZ-","execution":{"iopub.status.busy":"2023-10-02T04:04:26.310137Z","iopub.status.idle":"2023-10-02T04:04:26.310901Z","shell.execute_reply.started":"2023-10-02T04:04:26.310653Z","shell.execute_reply":"2023-10-02T04:04:26.310679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print_answer('5ad39d53604f3c001a3fe8d1')","metadata":{"id":"kSmsyhz04Kd8","execution":{"iopub.status.busy":"2023-10-02T04:04:26.312136Z","iopub.status.idle":"2023-10-02T04:04:26.312850Z","shell.execute_reply.started":"2023-10-02T04:04:26.312600Z","shell.execute_reply":"2023-10-02T04:04:26.312623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# metric = load_metric(\"squad_v2\" if impossible_answer else \"squad\")","metadata":{"id":"skSljAPi4MYf","execution":{"iopub.status.busy":"2023-10-02T04:04:26.314042Z","iopub.status.idle":"2023-10-02T04:04:26.314761Z","shell.execute_reply.started":"2023-10-02T04:04:26.314485Z","shell.execute_reply":"2023-10-02T04:04:26.314508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n# references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n# metric.compute(predictions=formatted_predictions, references=references)","metadata":{"id":"CYdjuUft4OAR","execution":{"iopub.status.busy":"2023-10-02T04:04:26.316054Z","iopub.status.idle":"2023-10-02T04:04:26.316808Z","shell.execute_reply.started":"2023-10-02T04:04:26.316540Z","shell.execute_reply":"2023-10-02T04:04:26.316564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using the Model with ðŸ¤— Pipeline","metadata":{"id":"uUEMtO0QpdbP"}},{"cell_type":"code","source":"from transformers import pipeline\n\nmodel_checkpoint = \"boimbukanbaim/distilbert-indonesian-squad\"\nquestion_answerer = pipeline(\"question-answering\", model=model_checkpoint)\n\ncontext = \"\"\"\nDalam matematika, turunan atau derivatif dari sebuah fungsi adalah cara mengukur sensitivitas perubahan nilai fungsi terhadap perubahan pada nilai variabelnya. Sebagai contoh, turunan dari posisi sebuah benda bergerak terhadap waktu mengukur kecepatan benda bergerak ketika waktu berjalan. Turunan adalah alat penting dalam kalkulus.\n\nTurunan sebuah fungsi satu variabel di suatu titik, jika itu ada, adalah kemiringan dari garis singgung dari grafik fungsi di titik tersebut. Garis singgung adalah hampiran (aproksimasi) linear terbaik dari fungsi di sekitar titik tersebut. Konsep turunan dapat diperumum untuk fungsi multivariabel. Dalam perumuman ini, turunan dianggap sebagai transformasi linear, dengan translasi yang sesuai, menghasilkan hampiran linear dari grafik fungsi multivariabel tersebut. Matriks Jacobi adalah matriks yang merepresentasikan transformasi linear terhadap suatu basis yang ditentukan. Matriks ini dapat ditentukan dengan turunan parsial dari variabel-variabel independen. Pada fungsi multivariabel bernilai real, matriks Jacobi tereduksi menjadi vektor gradien.\n\nProses menemukan turunan disebut diferensiasi. Kebalikan proses ini disebut dengan antiturunan. Teorema fundamental kalkulus menyatakan hubungan diferensiasi dengan integrasi. Turunan dan integral adalah dua operasi dasar dalam kalkulus satu-variabel.\n\nKonsep turunan fungsi yang universal banyak digunakan dalam berbagai cabang matematika maupun bidang ilmu yang lain. Dalam bidang ekonomi, turunan digunakan untuk menghitung biaya marginal, total penerimaan, dan biaya produksi. Bidang biologi menggunakan turunan untuk menghitung laju pertumbuhan mikroorganisme, dalam bidang fisika untuk menghitung kepadatan kawat, dalam bidang kimia untuk menghitung laju pemisahan, dalam bidang geografi untuk menghitung laju pertumbuhan penduduk, dan masih banyak lagi.\n\"\"\"\nquestion = \"Apa itu derivatif?\"\nquestion_answerer(question=question, context=context)","metadata":{"id":"Ch-remjypdbQ","outputId":"d81707d0-0718-404b-827b-f84b4c1c12cf","execution":{"iopub.status.busy":"2023-10-02T04:04:26.318145Z","iopub.status.idle":"2023-10-02T04:04:26.318815Z","shell.execute_reply.started":"2023-10-02T04:04:26.318578Z","shell.execute_reply":"2023-10-02T04:04:26.318600Z"},"trusted":true},"execution_count":null,"outputs":[]}]}